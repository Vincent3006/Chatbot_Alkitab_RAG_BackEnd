{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database 'chroma_db_Semantic' sudah ada. Proses pembuatan database baru dibatalkan.\n",
            "\n",
            "--- Verifikasi: Menampilkan contoh chunk ---\n",
            "Total dokumen dalam database: 150\n",
            "\n",
            "--- 10 Chunk Pertama ---\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Sumber: Kejadian 1:1:-Kejadian 1:1:\n",
            "Teks: Pada mulanya Allah \n",
            "menciptakan langit dan bumi....\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Sumber: Kejadian 1:2:-Kejadian 1:2:\n",
            "Teks: Bumi belum \n",
            "berbentuk dan kosong; gelap gulita \n",
            "menutupi samudera raya, dan Roh \n",
            "Allah melayang -layang di atas \n",
            "permukaan air....\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Sumber: Kejadian 1:3:-Kejadian 1:3:\n",
            "Teks: Berfirmanlah Allah: \n",
            "\"Jadilah terang.\" Lalu terang itu jadi....\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Chunk 4 ---\n",
            "Sumber: Kejadian 1:4:-Kejadian 1:4:\n",
            "Teks: Allah melihat bahwa \n",
            "terang itu baik, lalu dipisahkan -\n",
            "Nyalah terang itu dari gelap....\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Chunk 5 ---\n",
            "Sumber: Kejadian 1:5:-Kejadian 1:5:\n",
            "Teks: Dan Allah menamai \n",
            "terang itu siang, dan gelap itu \n",
            "malam. Jadilah petang dan jadilah \n",
            "pagi, itulah hari pertama....\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Chunk 6 ---\n",
            "Sumber: Kejadian 1:6:-Kejadian 1:7:\n",
            "Teks: Berfirmanlah Allah: \n",
            "\"Jadilah cakrawala di tengah segala \n",
            "air untuk memisahkan air dari air.\" Maka Allah \n",
            "menjadikan cakrawala dan Ia \n",
            "memisahkan air yang ada di bawah \n",
            "cakrawala itu dari air yang ada...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Chunk 7 ---\n",
            "Sumber: Kejadian 1:8:-Kejadian 1:8:\n",
            "Teks: Lalu Allah menamai \n",
            "cakrawala itu langit. Jadilah petang  Kejadian  \n",
            " \n",
            "dan jadilah pagi, itulah hari kedua....\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Chunk 8 ---\n",
            "Sumber: Kejadian 1:9:-Kejadian 1:9:\n",
            "Teks: Berfirmanlah Allah: \n",
            "\"Hendaklah segala air yang di bawah \n",
            "langit berkumpul pada satu tempat, \n",
            "sehingga kelihatan yang kering.\" \n",
            "Dan jadilah demikian....\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Chunk 9 ---\n",
            "Sumber: Kejadian 1:10:-Kejadian 1:10:\n",
            "Teks: Lalu Allah menamai \n",
            "yang kering itu darat, dan kumpulan \n",
            "air itu dinamai -Nya laut. Allah \n",
            "melihat bahwa semuanya itu baik....\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Chunk 10 ---\n",
            "Sumber: Kejadian 1:11:-Kejadian 1:12:\n",
            "Teks: Berfirmanlah Allah: \n",
            "\"Hendaklah tanah menumbuhkan \n",
            "tunas -tunas muda, tumbuh -\n",
            "tumbuhan yang berbiji, segala jenis \n",
            "pohon buah -buahan yang \n",
            "menghasilkan buah yang berbiji, \n",
            "supaya ada tumbuh -tumbuha...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Chunk Terakhir ---\n",
            "Sumber: Kejadian 8:3:-Kejadian 8:3:\n",
            "Teks: dan makin surutlah air \n",
            "itu dari muka bumi. Demikianlah \n",
            "berkurang air itu sesudah seratus \n",
            "lima puluh hari....\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import chromadb\n",
        "import nltk\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='PyPDF2')\n",
        "\n",
        "KNOWLEDGE_BASE_DIR = \"knowledge-base\"\n",
        "DB_NAME = \"chroma_db_Semantic\"\n",
        "HEADER_CROP_PERCENTAGE = 0.15 \n",
        "MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
        "MODEL_KWARGS = {'device': 'cpu'}\n",
        "ENCODE_KWARGS = {'normalize_embeddings': False}\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def extract_text_with_metadata(pdf_path: str) -> list:\n",
        "    data = []\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        raw_text = \"\"\n",
        "        if len(reader.pages) > 2:\n",
        "            for page in reader.pages[1:-1]:\n",
        "                raw_text += page.extract_text()\n",
        "        else:\n",
        "            print(f\"  [INFO] PDF '{os.path.basename(pdf_path)}' memiliki <= 2 halaman, dilewati.\")\n",
        "            return []\n",
        "        sentences = re.split(r'(\\w+\\s\\d+:\\d+:\\s)', raw_text)\n",
        "        for i in range(1, len(sentences), 2):\n",
        "            source = sentences[i].strip()\n",
        "            text = sentences[i+1].strip()\n",
        "            if text:\n",
        "                data.append({\"text\": text, \"source\": source})\n",
        "    except Exception as e:\n",
        "        print(f\"  [ERROR] Gagal memproses file {os.path.basename(pdf_path)}: {e}\")\n",
        "    return data\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def process_data(data, hf_embeddings, fixed_threshold=0.4, c=0.7, init_constant=2.0):\n",
        "    if not data:\n",
        "        return []\n",
        "    texts = [d['text'] for d in data]\n",
        "    sources = [d['source'] for d in data]\n",
        "    embeddings = np.array(hf_embeddings.embed_documents(texts))\n",
        "    chunks = []\n",
        "    current_chunk_texts = [texts[0]]\n",
        "    current_chunk_sources = [sources[0]]\n",
        "    cluster_start, cluster_end = 0, 1\n",
        "    pairwise_min = -float('inf')\n",
        "    for i in range(1, len(texts)):\n",
        "        cluster_embeddings = embeddings[cluster_start:cluster_end]\n",
        "        if cluster_end - cluster_start > 1:\n",
        "            new_sentence_similarities = cosine_similarity(embeddings[i].reshape(1, -1), cluster_embeddings)[0]\n",
        "            adjusted_threshold = pairwise_min * c * sigmoid((cluster_end - cluster_start) - 1)\n",
        "            new_sentence_similarity = np.max(new_sentence_similarities)\n",
        "            pairwise_min = min(np.min(new_sentence_similarities), pairwise_min)\n",
        "        else:\n",
        "            adjusted_threshold = 0\n",
        "            similarity_to_first = cosine_similarity(embeddings[i].reshape(1, -1), cluster_embeddings)[0][0]\n",
        "            pairwise_min = similarity_to_first\n",
        "            new_sentence_similarity = init_constant * pairwise_min\n",
        "            \n",
        "        if new_sentence_similarity > max(adjusted_threshold, fixed_threshold):\n",
        "            current_chunk_texts.append(texts[i])\n",
        "            current_chunk_sources.append(sources[i])\n",
        "            cluster_end += 1\n",
        "        else:\n",
        "            chunks.append({\n",
        "                \"document\": \" \".join(current_chunk_texts),\n",
        "                \"metadata\": {\"source_range\": f\"{current_chunk_sources[0]}-{current_chunk_sources[-1]}\"}\n",
        "            })\n",
        "            current_chunk_texts = [texts[i]]\n",
        "            current_chunk_sources = [sources[i]]\n",
        "            cluster_start, cluster_end = i, i + 1\n",
        "            pairwise_min = -float('inf')\n",
        "    chunks.append({\n",
        "        \"document\": \" \".join(current_chunk_texts),\n",
        "        \"metadata\": {\"source_range\": f\"{current_chunk_sources[0]}-{current_chunk_sources[-1]}\"}\n",
        "    })\n",
        "    return chunks\n",
        "if __name__ == \"__main__\":\n",
        "    if os.path.exists(DB_NAME):\n",
        "        print(f\"Database '{DB_NAME}' sudah ada. Proses pembuatan database baru dibatalkan.\")\n",
        "    else:\n",
        "        print(f\"Database '{DB_NAME}' tidak ditemukan. Memulai proses pembuatan database...\")\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except nltk.downloader.DownloadError:\n",
        "            print(\"  Mengunduh tokenizer NLTK (punkt)...\")\n",
        "            nltk.download('punkt')\n",
        "        print(\"  Menginisialisasi model embedding (Qwen)...\")\n",
        "        hf_embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=MODEL_NAME,\n",
        "            model_kwargs=MODEL_KWARGS,\n",
        "            encode_kwargs=ENCODE_KWARGS\n",
        "        )\n",
        "        client = chromadb.PersistentClient(path=DB_NAME)\n",
        "        collection = client.create_collection(\n",
        "            name=\"semantic_chunks\",\n",
        "            embedding_function=chromadb.utils.embedding_functions.SentenceTransformerEmbeddingFunction(model_name=MODEL_NAME)\n",
        "        )\n",
        "        pdf_files = [f for f in os.listdir(KNOWLEDGE_BASE_DIR) if f.endswith('.pdf')]\n",
        "        if not pdf_files:\n",
        "            print(f\"  [PERINGATAN] Tidak ada file PDF yang ditemukan di folder '{KNOWLEDGE_BASE_DIR}'.\")\n",
        "            exit()\n",
        "        for pdf_name in tqdm(pdf_files, desc=\"Memproses semua PDF\"):\n",
        "            pdf_path = os.path.join(KNOWLEDGE_BASE_DIR, pdf_name)\n",
        "            extracted_data = extract_text_with_metadata(pdf_path)\n",
        "            if not extracted_data:\n",
        "                continue\n",
        "            final_chunks = process_data(extracted_data, hf_embeddings)\n",
        "            if final_chunks:\n",
        "                ids = [f\"{pdf_name}_{i}\" for i in range(len(final_chunks))]\n",
        "                documents = [c['document'] for c in final_chunks]\n",
        "                metadatas = [c['metadata'] for c in final_chunks]\n",
        "                collection.add(\n",
        "                    documents=documents,\n",
        "                    metadatas=metadatas,\n",
        "                    ids=ids\n",
        "                )\n",
        "                print(f\"  Berhasil! {len(final_chunks)} potongan (chunks) ditambahkan dari {pdf_name}.\")\n",
        "    print(\"\\n--- Verifikasi: Menampilkan contoh chunk ---\")\n",
        "    if not os.path.exists(DB_NAME):\n",
        "        print(\"Database belum dibuat. Jalankan kembali script untuk membuatnya.\")\n",
        "    else:\n",
        "        client = chromadb.PersistentClient(path=DB_NAME)\n",
        "        try:\n",
        "            collection = client.get_collection(name=\"semantic_chunks\")\n",
        "            count = collection.count()\n",
        "            print(f\"Total dokumen dalam database: {count}\")\n",
        "            if count > 0:\n",
        "                results_first = collection.get(limit=10, include=[\"documents\", \"metadatas\"])\n",
        "                print(\"\\n--- 10 Chunk Pertama ---\")\n",
        "                for i, doc in enumerate(results_first['documents']):\n",
        "                    source = results_first['metadatas'][i].get('source_range', 'Sumber tidak diketahui')\n",
        "                    print(f\"\\n--- Chunk {i+1} ---\")\n",
        "                    print(f\"Sumber: {source}\")\n",
        "                    print(f\"Teks: {doc[:200]}...\") \n",
        "                    print(\"-\" * 60)\n",
        "                results_last = collection.get(limit=1, offset=count-1, include=[\"documents\", \"metadatas\"])\n",
        "                if results_last['documents']:\n",
        "                    print(\"\\n--- Chunk Terakhir ---\")\n",
        "                    doc = results_last['documents'][0]\n",
        "                    source = results_last['metadatas'][0].get('source_range', 'Sumber tidak diketahui')\n",
        "                    print(f\"Sumber: {source}\")\n",
        "                    print(f\"Teks: {doc[:200]}...\")\n",
        "                    print(\"-\" * 60)\n",
        "            else:\n",
        "                print(\"Database kosong.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Gagal mengambil data dari database: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello\n"
          ]
        }
      ],
      "source": [
        "print(\"hello\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
