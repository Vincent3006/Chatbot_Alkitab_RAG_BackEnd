{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import chromadb\n",
    "import nltk\n",
    "import pandas as pd \n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='PyPDF2')\n",
    "\n",
    "\n",
    "KNOWLEDGE_BASE_DIR = \"knowledge-base1\"\n",
    "DB_NAME = \"chroma_db_Semantic3\"\n",
    "HEADER_CROP_PERCENTAGE = 0.15 \n",
    "MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "MODEL_KWARGS = {'device': 'cpu'}\n",
    "ENCODE_KWARGS = {'normalize_embeddings': False}\n",
    "\n",
    "EXCEL_OUTPUT_FILENAME = \"Hasil_CHunk.xlsx\"\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Membersihkan teks \"\"\"\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_with_metadata(pdf_path: str) -> list:\n",
    "    \"\"\"Mengekstrak teks dari file PDF dan membaginya berdasarkan format sumber.\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        if not reader.pages:\n",
    "            print(f\"  [PERINGATAN] PDF '{os.path.basename(pdf_path)}' kosong atau tidak dapat dibaca.\")\n",
    "            return []\n",
    "        raw_text = \"\"\n",
    "        print(f\"  Mengekstrak teks dari {os.path.basename(pdf_path)} (semua halaman)...\")\n",
    "        for page in reader.pages:\n",
    "            original_height = page.mediabox.height\n",
    "            new_top = original_height * (1 - HEADER_CROP_PERCENTAGE)\n",
    "            page.cropbox.upper_y = new_top\n",
    "            raw_text += page.extract_text()\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] Gagal membaca file {os.path.basename(pdf_path)}: {e}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        sentences = re.split(r'(\\w+\\s\\d+:\\d+:\\s)', raw_text)\n",
    "        for i in range(1, len(sentences), 2):\n",
    "            source = sentences[i].strip()\n",
    "            text = sentences[i+1].strip()\n",
    "            if text:\n",
    "                cleaned_text = preprocess_text(text)\n",
    "                data.append({\"text\": cleaned_text, \"source\": source})\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] Gagal saat memecah teks dari {os.path.basename(pdf_path)}: {e}\")\n",
    "    return data\n",
    "\n",
    "def process_data_normal_semantic_chunking(data, hf_embeddings, similarity_threshold=0.7):\n",
    "    if not data:\n",
    "        return []\n",
    "    texts = [d['text'] for d in data]\n",
    "    sources = [d['source'] for d in data]\n",
    "    embeddings = np.array(hf_embeddings.embed_documents(texts))\n",
    "    chunks = []\n",
    "    current_chunk_texts = [texts[0]]\n",
    "    current_chunk_sources = [sources[0]]\n",
    "    for i in range(1, len(texts)):\n",
    "        last_chunk_embedding = embeddings[i-1].reshape(1, -1)\n",
    "        new_text_embedding = embeddings[i].reshape(1, -1)\n",
    "        similarity = cosine_similarity(last_chunk_embedding, new_text_embedding)[0][0]\n",
    "        if similarity >= similarity_threshold:\n",
    "            current_chunk_texts.append(texts[i])\n",
    "            current_chunk_sources.append(sources[i])\n",
    "        else:\n",
    "            chunks.append({\n",
    "                \"document\": \" \".join(current_chunk_texts),\n",
    "                \"metadata\": {\"source_range\": f\"{current_chunk_sources[0]}-{current_chunk_sources[-1]}\"}\n",
    "            })\n",
    "            current_chunk_texts = [texts[i]]\n",
    "            current_chunk_sources = [sources[i]]\n",
    "    if current_chunk_texts:\n",
    "        chunks.append({\n",
    "            \"document\": \" \".join(current_chunk_texts),\n",
    "            \"metadata\": {\"source_range\": f\"{current_chunk_sources[0]}-{current_chunk_sources[-1]}\"}\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(KNOWLEDGE_BASE_DIR):\n",
    "        print(f\"Error: Direktori '{KNOWLEDGE_BASE_DIR}' tidak ditemukan.\")\n",
    "        exit()\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except nltk.downloader.DownloadError:\n",
    "        nltk.download('punkt')\n",
    "        \n",
    "    hf_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=MODEL_NAME, model_kwargs=MODEL_KWARGS, encode_kwargs=ENCODE_KWARGS\n",
    "    )\n",
    "\n",
    "    print(f\"Menghubungkan atau membuat database '{DB_NAME}'...\")\n",
    "    client = chromadb.PersistentClient(path=DB_NAME)\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=\"semantic_chunks\",\n",
    "        embedding_function=chromadb.utils.embedding_functions.SentenceTransformerEmbeddingFunction(model_name=MODEL_NAME)\n",
    "    )\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(KNOWLEDGE_BASE_DIR) if f.endswith('.pdf')]\n",
    "    if not pdf_files:\n",
    "        print(f\"  Tidak ada file PDF yang ditemukan di folder '{KNOWLEDGE_BASE_DIR}'.\")\n",
    "        exit()\n",
    "\n",
    "    print(\"\\nMemulai penambahan data ke database...\")\n",
    "    for pdf_name in tqdm(pdf_files, desc=\"Memproses semua PDF\"):\n",
    "        existing_docs = collection.get(where={\"source_file\": pdf_name}, limit=1)\n",
    "        if existing_docs['ids']:\n",
    "            print(f\"\\n  [INFO] Melewati {pdf_name}, data sudah ada di database.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nMemproses file baru: {pdf_name}\")\n",
    "        pdf_path = os.path.join(KNOWLEDGE_BASE_DIR, pdf_name)\n",
    "        \n",
    "        extracted_data = extract_text_with_metadata(pdf_path)\n",
    "        if not extracted_data:\n",
    "            print(f\"  Tidak ada data yang diekstrak dari {pdf_name}.\")\n",
    "            continue\n",
    "            \n",
    "        final_chunks = process_data_normal_semantic_chunking(extracted_data, hf_embeddings, similarity_threshold=0.7) \n",
    "        \n",
    "        if final_chunks:\n",
    "            ids = [f\"{pdf_name}_{i}\" for i in range(len(final_chunks))]\n",
    "            documents = [c['document'] for c in final_chunks]\n",
    "            metadatas = []\n",
    "            for c in final_chunks:\n",
    "                meta = c['metadata']\n",
    "                meta['source_file'] = pdf_name\n",
    "                metadatas.append(meta)\n",
    "\n",
    "            collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "            print(f\"  Berhasil! {len(final_chunks)} potongan (chunks) dari {pdf_name} ditambahkan.\")\n",
    "            \n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PROSES PENAMBAHAN DATA SELESAI\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nMemulai ekspor keseluruhan database ke file Excel: {EXCEL_OUTPUT_FILENAME}\")\n",
    "    \n",
    "    try:\n",
    "        total_docs_in_db = collection.count()\n",
    "        if total_docs_in_db == 0:\n",
    "            print(\"Database kosong. Tidak ada data untuk diekspor.\")\n",
    "        else:\n",
    "            print(f\"Mengambil {total_docs_in_db} dokumen dari database...\")\n",
    "            all_data = collection.get(\n",
    "                limit=total_docs_in_db,\n",
    "                include=[\"documents\", \"metadatas\"]\n",
    "            )\n",
    "            \n",
    "            data_for_excel = []\n",
    "            for i in range(len(all_data['ids'])):\n",
    "                data_for_excel.append({\n",
    "                    \"Nomor\": i + 1,\n",
    "                    \"source_file\": all_data['metadatas'][i].get('source_file', 'Tidak Diketahui'),\n",
    "                    \"source_range\": all_data['metadatas'][i].get('source_range', 'Tidak Diketahui'),\n",
    "                    \"isi_ayat\": all_data['documents'][i]\n",
    "                })\n",
    "            \n",
    "            df = pd.DataFrame(data_for_excel)\n",
    "            \n",
    "            if os.path.exists(EXCEL_OUTPUT_FILENAME):\n",
    "                os.remove(EXCEL_OUTPUT_FILENAME)\n",
    "                print(f\"File '{EXCEL_OUTPUT_FILENAME}' yang sudah ada telah dihapus.\")\n",
    "                \n",
    "            df.to_excel(EXCEL_OUTPUT_FILENAME, index=False)\n",
    "            print(f\"\\nBerhasil! Seluruh data dari database telah diekspor ke '{EXCEL_OUTPUT_FILENAME}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Terjadi kesalahan saat mengekspor data ke Excel: {e}\")\n",
    "\n",
    "    print(\"\\n--- Program Selesai ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
